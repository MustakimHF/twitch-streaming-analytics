# ğŸ® Twitch Streaming Analytics (with Airflow ETL)

A **data engineering and analytics project** that ingests live data from the **Twitch API**, processes it with **Apache Airflow**, and stores insights in a **Postgres database**.  
It demonstrates **ETL (Extractâ€“Transformâ€“Load)** pipelines, **data quality checks**, **trend analysis**, and **visualisation** â€” finished with stakeholder-ready outputs.  

---

## ğŸš€ What This Project Does  

- ğŸ“¡ **Extracts live data** from Twitch (stream metadata, game categories, viewers)  
- âš™ï¸ **Automates ETL pipeline** with **Airflow DAGs** (scheduled runs, retries, monitoring)  
- ğŸ§¹ **Cleans and validates** data (deduplication, UTF-8 handling, schema enforcement)  
- ğŸ—„ï¸ **Loads into Postgres** for structured analytics  
- ğŸ“Š **Analyses streaming trends** by game, category, and viewers  
- ğŸ“ˆ **Produces visuals & summaries** (most-watched games, viewer trends, game diversity)  

---

## ğŸ§° Tech Stack  

- **Data Orchestration**: Apache Airflow (Docker Compose)  
- **Database**: PostgreSQL (with SQLAlchemy + psycopg2)  
- **Python**: `pandas`, `numpy`, `requests`, `python-dotenv`  
- **Visualisation**: `matplotlib`  
- **Containerisation**: Docker & docker-compose  
- **Config Management**: `.env` for Twitch API secrets  

---

## ğŸ“ Repository Structure  

```
twitch-streaming-analytics/
â”œâ”€â”€ README.md                   # Project overview (this file)
â”œâ”€â”€ docker-compose.yaml          # Services: Airflow + Postgres
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ twitch_etl_dag.py       # Main Airflow DAG
â”‚   â”œâ”€â”€ extract_twitch.py       # Extract: Twitch API calls
â”‚   â”œâ”€â”€ transform.py            # Transform: clean & enrich data
â”‚   â””â”€â”€ load.py                 # Load: push to Postgres
â”œâ”€â”€ scripts/                    # Standalone ETL scripts
â”‚   â”œâ”€â”€ auth.py                 # Twitch API authentication
â”‚   â”œâ”€â”€ extract_twitch.py       # Extract: Twitch API calls
â”‚   â”œâ”€â”€ transform.py            # Transform: clean & enrich data
â”‚   â”œâ”€â”€ load_db.py              # Load: push to SQLite
â”‚   â””â”€â”€ run_etl.py              # Complete ETL pipeline
â”œâ”€â”€ analysis/                   # Analysis scripts
â”‚   â”œâ”€â”€ top_games.py            # Top games analysis
â”‚   â”œâ”€â”€ peak_hours.py           # Peak hours analysis
â”‚   â””â”€â”€ weekend_analysis.py     # Weekend vs weekday analysis
â”œâ”€â”€ db/
â”‚   â””â”€â”€ twitch.db               # SQLite database
â”œâ”€â”€ data/                       # Raw + processed CSVs
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ plots/                  # Generated plots (PNG)
â”œâ”€â”€ .env.example                # Example environment variables
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## â–¶ï¸ How to Run (Local, Python + SQLite)

### 1. Create a virtual environment  

**Windows PowerShell**
```powershell
python -m venv venv
venv\Scripts\Activate.ps1
```

**macOS/Linux**
```bash
python3 -m venv venv
source venv/bin/activate
```

---

### 2. Install dependencies  
```bash
pip install -r requirements.txt
```

---

### 3. Configure environment variables  
Copy the example file and add your Twitch API credentials:  

**Windows**
```powershell
copy .env.example .env
```

**macOS/Linux**
```bash
cp .env.example .env
```

Edit `.env`:
```env
TWITCH_CLIENT_ID=your_twitch_client_id
TWITCH_CLIENT_SECRET=your_twitch_client_secret
```

---

### 4. Run the ETL pipeline (local, SQLite target)  
```bash
python scripts/run_etl.py
```
âœ… Produces:  
- `data/raw/streams_raw.csv` (raw API data)  
- `data/processed/streams_processed.csv` (cleaned/enriched)  
- `db/twitch.db` (SQLite database with `streams` table)  

---

### 5. Run analysis scripts  
```bash
python analysis/top_games.py
python analysis/peak_hours.py
```
âœ… Produces plots in `outputs/plots/`:
- `top_games.png` â€“ Top games by viewers  
- `peak_hours.png` â€“ Hourly viewing trends  

---

## â–¶ï¸ How to Run (Docker + Airflow + Postgres)

### 1. Start Docker services  
```bash
docker compose up -d
```

- Airflow web UI â†’ [http://localhost:8080](http://localhost:8080)  
- Postgres DB â†’ `localhost:5432`  

Login to Airflow:  
- **Username**: `airflow`  
- **Password**: `airflow`  

---

### 2. Trigger the ETL DAG  
- Open the Airflow UI.  
- Enable the `twitch_etl_dag`.  
- Monitor tasks: `extract â†’ transform â†’ load`.  
- Data lands in Postgres `streams` table.  

---

### 3. Run analysis scripts  
Even when using Docker/Postgres, you can run the same analysis scripts locally:  

```bash
python analysis/top_games.py
python analysis/peak_hours.py

```

Outputs â†’ `outputs/plots/`

---

## ğŸ“Š Example Visuals  

**Top Games by Average Viewers**  
![Top Games](outputs/plots/top_games.png)  

**Peak Hours Analysis**  
![Peak Hours](outputs/plots/peak_hours_analysis.png)  

### PowerBI example visual
<img width="1279" height="717" alt="image" src="https://github.com/user-attachments/assets/c9b39990-2161-4881-b0c4-190294cbf88e" />

---

## ğŸ¯ Why This Project Matters  

This project showcases **real-world data engineering**:  

- **ETL pipelines** with Airflow for reliability, retries, and scheduling  
- **API integration** with Twitchâ€™s OAuth flow and rate limits  
- **Database design & SQL analytics** for structured insights  
- **Exploratory analysis & visualisation** for stakeholder reporting  
- **Containerised deployment** for reproducibility (Docker)  

---

## ğŸ”’ Notes  

- `.env` is git-ignored â€” never commit secrets  
- Airflow uses a Postgres backend (persistent via Docker volume)  
- Extend project: add sentiment analysis on Twitch chat logs, or track streamer growth over time  
